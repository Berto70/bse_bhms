{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math as m\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "\n",
    "import hvplot.dask\n",
    "import hvplot.pandas\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ce81d",
   "metadata": {},
   "source": [
    "Use Bayesian inference with a prior that is based on our knowledge (SEVN data), and update it using the real data (likelihood) to obtain the posterior distribution. It is important to choose a prior that is not too informative or too diffuse. \n",
    "\n",
    "Learning the probability distribution from the simulations, you can use statistical techniques such as maximum likelihood estimation or kernel density estimation. You could use MLE or KDE to estimate the probability distribution of the features in a dataset, and then use this probability distribution to train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the parquet files\n",
    "\n",
    "file_int = pd.read_parquet('./data/sevn_output_Z0.0001A1L1/int_binsys.parquet')\n",
    "file_nint = pd.read_parquet('./data/sevn_output_Z0.0001A1L1/non_int_binsys.parquet')\n",
    "real_data = pd.read_parquet('./real_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a368e",
   "metadata": {},
   "source": [
    "# 1st method: Dirichlet pmm + Gaussian regressor + squared-exponential kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e6fa3",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "The Dirichlet process is a stochastic proces used in Bayesian nonparametric models of data, particularly in Dirichlet process mixture models (also known as infinite mixture models). It is a distribution over distributions, i.e. each draw from a Dirichlet process is itself a distribution. It is called a Dirichlet process because it has Dirichlet distributed finite dimensional marginal distributions, just as the Gaussian process, another popular stochastic process used for Bayesian nonparametric regression, has Gaussian distributed finite dimensional marginal distributions. Distributions drawn from a Dirichlet process are discrete, but cannot be described using a finite number of parameters, thus the classification as a nonparametric model.\n",
    "\n",
    "#### Motivation and background\n",
    "\n",
    "The Bayesian nonparametric approach is an alternative to parametric modeling and selection. By using a model with an unbounded complexity, underfitting is mitigated, while the Bayesian approach of computing or approximating the full posterior over parameters mitigates overfitting.\n",
    "Unfortunately the Dirichlet process is limited by the fact that draws from it are discrete distributions, and generalizations to more general priors did not have tractable posterior inference until the development of MCMC techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm #for bayesian approach\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor \n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Load simulated and real data\n",
    "sim_data = pd.read_csv(\"sim_data.csv\")\n",
    "real_data = pd.read_csv(\"real_data.csv\")\n",
    "\n",
    "# Define list of feature pairs\n",
    "feature_pairs = [[\"Mass_BH\", \"Mass_1\"], [\"Mass_BH\", \"logP\"], [\"Mass_BH\", \"Eccentricity\"], [\"Mass_1\", \"logP\"], [\"Mass_1\", \"Eccentricity\"], [\"logP\", \"Eccentricity\"]]\n",
    "\n",
    "# Define dictionary to store learned probability distributions\n",
    "pdf_dict = {}\n",
    "\n",
    "# Loop over feature pairs\n",
    "for feature_pair in feature_pairs:\n",
    "    # Select features from simulated data\n",
    "    sim_features = sim_data[feature_pair].values\n",
    "\n",
    "    # Define Bayesian inference model using Dirichlet process mixture model\n",
    "    with pm.Model() as model:\n",
    "        # Define prior distribution using Dirichlet process mixture model\n",
    "        alpha = pm.Gamma(\"alpha\", 1, 1)\n",
    "        G = pm.NormalMixture(\"G\", w=np.ones(len(sim_features))/len(sim_features), mu=sim_features, sigma=np.ones(len(sim_features)), comp_shape=len(sim_features))\n",
    "        prior = pm.DensityDist(\"prior\", lambda x: pm.math.logsumexp(pm.Normal.dist(mu=G, sigma=1/alpha).logp(x) + pm.math.log(G.shape[0]), axis=0) - pm.math.log(G.shape[0]))\n",
    "\n",
    "        # Define likelihood\n",
    "        likelihood = pm.DensityDist(\"likelihood\", lambda x: prior.logp(x), observed=real_data[feature_pair].values)        \n",
    "        ## A distribution that can be used to wrap black-box log density functions.\n",
    "\n",
    "        # Define posterior\n",
    "        posterior = pm.DensityDist(\"posterior\", lambda x: prior.logp(x) + likelihood.logp(x))\n",
    "\n",
    "        # Sample from posterior using MCMC\n",
    "        trace = pm.sample(1000)\n",
    "\n",
    "    # Compute posterior predictive distribution\n",
    "    ppc = pm.sample_posterior_predictive(trace, samples=1000, model=model)\n",
    "\n",
    "    # Use posterior predictive distribution to learn probability distribution\n",
    "    X = sim_features\n",
    "    y = ppc[\"likelihood\"].mean(axis=0)\n",
    "    kernel = RBF(length_scale=1)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel)\n",
    "    gp.fit(X, y)\n",
    "\n",
    "    # Store learned probability distribution in dictionary\n",
    "    pdf_dict[tuple(feature_pair)] = gp\n",
    "    \n",
    "    \n",
    "# Use updated joint probability distribution as prior in machine learning algorithm\n",
    "# TODO: add code for machine learning algorithm here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15042782",
   "metadata": {},
   "source": [
    "# 2nd method: KDE + Bayesian approach  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f7fbc",
   "metadata": {},
   "source": [
    "In statistics, kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights. KDE answers a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n",
    "\n",
    "The bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate. The most common optimality criterion used to select this parameter is the expected L2 risk function, also termed the mean integrated squared error.\n",
    "\n",
    "##### Multivariate KDE\n",
    "\n",
    "The choice of the kernel function K is not crucial to the accuracy of kernel density estimators. On the other hand, the choice of the bandwidth matrix H is the single most important factor affecting its accuracy since it controls the amount and orientation of smoothing induced.\n",
    "\n",
    "This leads to the choice of the parametrisation of this bandwidth matrix. The three main parametrisation classes (in increasing order of complexity) are S, the class of positive scalars times the identity matrix; D, diagonal matrices with positive entries on the main diagonal; and F, symmetric positive definite matrices. The S class kernels have the same amount of smoothing applied in all coordinate directions, D kernels allow different amounts of smoothing in each of the coordinates, and F kernels allow arbitrary amounts and orientation of the smoothing. Historically S and D kernels are the most widespread due to computational reasons, but research indicates that important gains in accuracy can be obtained using the more general F class kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import pymc3 as pm\n",
    "\n",
    "# Load data\n",
    "sim_data = pd.read_csv('sim_data.csv')\n",
    "real_data = pd.read_csv('real_data.csv')\n",
    "\n",
    "# Define pairs of features to use\n",
    "feature_pairs = [('Mass_BH', 'Mass_1'), ('Mass_BH', 'logP'), ('Mass_BH', 'Eccentricity'),\n",
    "                 ('Mass_1', 'logP'), ('Mass_1', 'Eccentricity'), ('logP', 'Eccentricity')]\n",
    "\n",
    "# Estimate joint probability distribution for each feature pair in simulated data\n",
    "kdes = {}\n",
    "for pair in feature_pairs:\n",
    "    data_pair = sim_data[list(pair)]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(data_pair) # try to use `statsmodels.nonparametric.kernel_density.KDEMultivariate`\n",
    "    kdes[pair] = kde\n",
    "\n",
    "# Define Bayesian model\n",
    "with pm.Model() as model:\n",
    "    # Define priors for each feature pair based on KDE estimates\n",
    "    priors = {}\n",
    "    for pair in feature_pairs:\n",
    "        kde = kdes[pair]\n",
    "        priors[pair] = pm.DensityDist(pair, lambda x: kde.score_samples(x.reshape(1, -1)))\n",
    "\n",
    "    # Define likelihood based on real data\n",
    "    likelihood = pm.Potential('likelihood', sum([priors[pair](real_data[list(pair)].iloc[0].values) for pair in feature_pairs]))\n",
    "    ##The Potential function is used to add arbitrary factors (such as constraints or other likelihood components) to adjust the probability density of the model.\n",
    "\n",
    "    # Run inference\n",
    "    trace = pm.sample(1000, chains=1, target_accept=0.9)\n",
    "\n",
    "# Use updated joint probability distribution as prior in machine learning algorithm\n",
    "# TODO: add code for machine learning algorithm here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b03bf",
   "metadata": {},
   "source": [
    "## Example of ML algorithm: it's a mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use updated joint probability distribution as prior in machine learning algorithm\n",
    "# Define feature matrix using the joint probability distribution\n",
    "X = []\n",
    "for index, row in real_data.iterrows():\n",
    "    x = []\n",
    "    for pair in feature_pairs:\n",
    "        kde = kdes[pair]\n",
    "        x.append(kde.score_samples(row[list(pair)].values.reshape(1, -1))[0])\n",
    "    X.append(x)\n",
    "\n",
    "# Define target variable\n",
    "y = ['BH+MS' for i in range(len(real_data))]  ## HERE a major issue is raised\n",
    "\n",
    "# Train machine learning model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict probabilities for new data\n",
    "new_data = pd.read_csv('new_data.csv')\n",
    "X_new = []\n",
    "for index, row in new_data.iterrows():\n",
    "    x_new = []\n",
    "    for pair in feature_pairs:\n",
    "        kde = kdes[pair]\n",
    "        x_new.append(kde.score_samples(row[list(pair)].values.reshape(1, -1))[0])\n",
    "    X_new.append(x_new)\n",
    "y_new = clf.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae2c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
